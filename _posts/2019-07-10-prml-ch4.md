---
layout: post
title: "PRML Ch.4 - linear models for classification"
categories: PRML
author: lee gunjun
---

decision boundary 를 통해 decision region 들로 나눔.

1장에서 classification 문제를 푸는 3가지 방법에 대해 알아봤다.

1.  simple
    - x 가 input 으로 들어오면 바로 특정 class 에 배정.
2.  discriminative
    - $p(C_k \vert x)$ 를 model. 
3. generative
    - $p(x \vert C_k)$ 와 $p(C_k)$ model 후 bayes rule 이용.

이번 chapter 에서 이들을 전부 배워볼 것임

regression 에서는 output 이 real 이면 됐는데 classification 에서는 (0, 1) 이어야 함.

$y(x) = f(w^T x+w_0)$. f는 nonlinear 이고 (0, 1) 로 만들어주는 함수다. 이를 activation function 이라 한다.

먼저 두개의 class 에서 생각하자.

$y(x) = w^T x + w_0$ 여기서 w는 weight 라 하고 $w_0$ 는 bias. decision boundary 는 $y(x) = 0$. 

multi class 생각하자.

$y_k(x) = w_k^T x + w_{k0}$ 를 각 class $C_k$ 마다 만들고, 저 값이 가장 큰 class 에 x를 배정하면 된다. decision boundary 는 $y_k(x) = y_j(x)$ 가 된다.

이 선형 함수의 parameter을 학습하는 방법 3가지에 대해 알아보자.

**lsm**

각 class $C_k$ 를 linear model 로 표현하자 $y_k(x) = w_k^T x + w_{k0}$

하나의 식으로 쓰면 $y(x) = \tilde{W}^T \tilde{x}$ 이고

이제 $\tilde{W}$ 를 알아내기 위한 err를 정의하자. 

mean square error 를 다음과 같이 구할 수 있다.

$E_D(\tilde{W}) = \frac{1}{2} Tr \{ (\tilde{X}\tilde{W} - T)^T (\tilde{X}\tilde{W} - T) \}$

미분해서 해를 구하면 $\tilde{W} = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T T$

근데 lsm 을 써서 위와 같이 w를 구하는 것에는 문제가 있다. 먼저 outlier 에 너무 민감하다(그림 4.4). 그리고 k=3 인 예제에 적용해보면 매우 엉망인 결과를 뱉는다(그림 4.5). 

후자는 사실 어쩌면 당연하다. mean square 를 이용하는 풀이가 target이 gaussian 분포를 이루고 있다는 가정에서 나온 건데 이는 target 값이 binary인 classification 문제에는 적합하지 않다.


**Fisher's linear discriminant**

dimension reduction 의 관점으로 보자. D 차원의 input vector x 를 생각하자. $y = w^T x$. threshold $-w_0$ 두고, $y \ge -w_0$ 면 class 1 에, 아니면 class 2 에 넣자. D 차원을 1차원에 투영하고 classify 하므로 당연히 잘 안되지만, 그중에서도 잘되는 $w^T$ 를 선택할 수 있을 것이다.

각 class 의 평균 vector를 구해보자

$m_1 = \frac{1}{N_1}\sum_{n \in C_1} x_n,\ m_2 = \frac{1}{N_2} \sum_{n \in C_2} x_n$

w에 투영했을 때 class 간 분리 정도를 측정하는 가장 쉬운 방법은 투영 된 class 들의 평균이 얼마나 멀리 있는지 보는 것이다. w가 단위 벡터라 하자. 

그런데 당연히 이런 방식에는 문제가 있다. 그림 4.6 참조. 잘 구별되던게 위와 같은 방법으로 w를 찾아서 하니까 더 안된다. 그래서 w 에 투영한 다음 분산까지 고려하여 class 간 평균은 멀게, 분산은 작게 하는 방법을 찾아보자.

각 x가 w에 투영된 점을 y라 하자

각 class 별로 분산을 계산하자

$s_k^2 = \sum_{n \in C_k} (y_n-m_k)^2,\ y_n = w^T x$ 

fisher criterion: $J(w)$ 를 새로 정의할 것이다. $J(w) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}$ 이를 풀어쓰면 $J(w) = \frac{w^T S_B w}{w^T S_W w}$ 가 된다. 

$S_B$ 는 inter-class 공분산 행렬 $S_B = (m_2-m_1)(m_2-m_1)^T$, 

$S_W$ 는 within class 공분산 행렬이다. $S_W = \sum_{n \in C_1}(x_n - m_1)(x_n - m_1)^T + \sum_{n \in C_2}(x_n - m_2)(x_n - m_2)^T$

$(w^T S_M w)S_W w = (w^T  S_W w)S_B w$ 우린 방향만 알면 되므로 scalar 날려 버리면 $w \propto S_W^{-1}(m_2-m_1)$ 이 된다.

lsm 과 fissure criterion 의 관계를 알아보자. 관계 없어 보이지만 사실 fissure criterion 은 lsm 의 special case 라 볼 수 있다.

우리는 lsm 에서 target를 one-hot vector 로 사용했는데, $C_1$ 의 target을 $\frac{N}{N_1}$, $C_2$의 target를 $-\frac{N}{N_2}$ 라 하면 fissure 와 동일한 해가 나온다.

lsm: $E = \frac{1}{2} \sum_{n=1}^N (w^T x_n + w_0 - t_n)^2$ -> 해 구하면 fissure 와 같음

multi class 에서 fissure criterion를 알아보자.

K 개 클래스. input dim: $D$. $D'$ 개의 $y_k = w_k^T x,\ k=1, \cdots, D'$ 이용. bias term 없음.

그냥 뭐 어떻게 잘 풀면 됨.

----

perceptron

$y(x) = f\left(w^T \phi\left(x\right) \right)$, $f(x) = \begin{cases}1&a\ge0 \\ -1& a\lt 0\end{cases}$

$C_1$는 t=1, $C_2$는 t=-1 로 표현.

w는 loss function 최소화해서 구함. 쉽게 생각되는 loss function 은 오분류 sample 개수다. 근데 이는 differentiable 하지 않아서 별로임. 

그러니 perceptron criterion 이라는 loss function 소개함.

$E_p(w) = - \sum_{n \in M} w^T \phi_n t_n$, $M$은 오분류된 sample들

----

Probabilistic Generative models

먼저 generative 방식으로 풀어보자

$p(x \vert C_k)$, $p(C_k)$ 를 model 하고 posterior 계산

class 2 개일 때

$$\begin{matrix}
p(C_1 \vert x) &=& \frac{p(x \vert C_1) p(C_1)}{p(x \vert C_1) p(C_1)+p(x \vert C_2) p(C_2)} \\
&=& \frac{1}{1+\exp(-a)} \\
&=& \sigma(a)
\end{matrix}$$
  
여기서 

$$a = \log \frac{p(x \vert C_1) p (C_1)}{p(x \vert C_2) p (C_2)}$$

sigmoid 가 나왔다!

class k 개일 떄

$$\begin{matrix}
p(C_k \vert x) &=& \frac{p(x \vert C_k)p(C_k)}{\sum_j p(x \vert C_j)p(C_j)} \\
&=& \frac{\exp(a_k)}{\sum_j \exp(a_j)}
\end{matrix}$$

여기서

$$a_k = \log p(x \vert C_k) p(C_k)$$

softmax 가 나왔다!

----

$p(x \vert C_k)$ 가 gaussian 이라 가정하고, 공분산 행렬 공유한다고 가정하면

$p(x \vert C_k) = \frac{1}{(2\pi)^{D / 2}} \frac{1}{\left\vert \Sigma \right\vert^{1/2}} \exp \{ -\frac{1}{2} (x- \mu_k)^T \Sigma^{-1} (x- \mu_k) \}$

class 가 2 개일 때 전에 구한 sigmoid 에 넣으면

$p(C_1 \vert x) = \sigma(w^T x + w_0)$ 를 얻을 수 있다. 즉 sigmoid 안의 값이 x에 대한 선형함수가 되었다. 따라서 decision boundary 는 linear 가 된다.

class 가 K 개일 때는

$a_k(x) = w_k^T x + w_{k0}$ 가 된다.

공분산이 공유되지 않으면 boundary 는 quadratic 이 된다.

----

$p(x \vert C_k)$ 형태를 명시하고 mle 를 쓰면 $p(x \vert C_k)$의 parameter들과 $p(C_k)$ 를 구할 수 있다.


class 가 2개, $p(C_1) = \pi$ 라 하자. 

$p(x_n, C_1) = p(C_1)p(x_n \vert C_1) = \pi \mathcal{N} (x_n \vert \mu_1, \Sigma)$

$p(x_n, C_2) = p(C_2)p(x_n \vert C_2) = (1-\pi) \mathcal{N} (x_n \vert \mu_2, \Sigma)$

따라서 likelihood: $p(t, x \vert \pi, \mu_1, \mu_2, \Sigma) = \prod_{n=1}^N \left[ \pi \mathcal{N}(x_n \vert \mu_1, \Sigma) \right]^{t_n} \left[ (1-\pi) \mathcal{N} (x_n \vert \mu_2, \Sigma)^{1-t_n} \right]$

특이한 점은 likelihood 가 t와 x 둘 다에 대해서 함.

mle 로 $\pi$ 구해보면 $\pi =\frac{N_1}{N_1 + N_2}$, 평균들은 $\mu_1 = \frac{1}{N_1} \sum_{n=1}^N t_n x_n,\ \mu_2 = \frac{1}{N_2} \sum_{n=1}^N (1-t_n)x_n$, 공분산은 복잡하니 생략

k 개 class 확장도 쉽게 됨


