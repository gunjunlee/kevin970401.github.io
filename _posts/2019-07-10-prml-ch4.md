---
layout: post
title: "PRML Ch.4 - linear models for classification"
categories: PRML
author: lee gunjun
---

# intro

ch3 에서 regression 을 다뤘다. 이번에는 classification 에 대해 다뤄보자. 

classification 에서 input space 는 decision boundary 를 통해 decision region 들로 나눈다. 

우리는 linear model 을 통해 classification 문제를 해결할 것이다

우리는 이미 ch1 에서 classification 문제를 푸는 방법에 3가지를 알아봤다.

1.  simple
    - x 가 input 으로 들어오면 바로 특정 class 에 배정.
2.  discriminative
    - $p(C_k \vert x)$ 를 model. 
3. generative
    - $p(x \vert C_k)$ 와 $p(C_k)$ model 후 bayes rule 이용.

이번 chapter 에서 이들을 전부 배워볼 것이다.

regression 에서는 output 이 그저 real value 면 됐는데 classification 에서는 binary class label 을 예측하므로 (0, 1) 에 속하는 posterior 를 구하고 싶다. 이를 위해 다음과 같은 nonlinear function f 를 사용한다.

$$y(x) = f(w^T x+w_0)$$

f 를 activation function 이라 부른다.

# 1. Discriminant functions

Discriminant functions 은 input vector $x$ 를 k 개의 클래스 중 하나에 배정하는 함수다.

## 1.1 두 개의 class 일 때

$$y(x) = w^T x + w_0$$

여기서 $w$는 weight, $w_0$ 는 bias 라 한다. decision boundary 는 $y(x) = 0$

즉 $y(x) \ge 0$ 일 때는 class 1 에, $y(x) \le 0$ 일 때는 class 2 에 대정한다.


## 1.2 multi class 일 때

여러개의 binary class 를 판별하는 함수를 합쳐서 K 개의 class 를 판별하는 방법이 있을 수 있다.

이 방법엔 크게 두가지가 있는데, 둘다 문제가 있다

먼저 one-versus-the-rest 는 특정 클래스 $C_k$ 에 속하는지, 그렇지 않는지를 판별하는 분류기를 K-1 개 사용한다.

그리고 one-versus-one 은 $K(K-1)/2$ 개의 모든 class 쌍에 대해 binary classification 을 하는 것이다. 

이 두 방법은 아래의 그림과 같이 불확실한 영역은 가지는 문제를 겪는다

![](/assets/images/prml2/multi_class.png)

이러한 문제를 해결할 수 있는 방법이 있다. linear function 을 각 class 마다 만들면 된다.

$$y_k(x) = w_k^T x + w_{k0}$$

저 linear function 의 값이 가장 큰 class 에 x를 배정하면 된다. decision boundary 는 $y_k(x) = y_j(x)$ 가 될 것이다.

이 선형 함수의 parameter을 학습하는 방법 3가지에 대해 알아보자: Least square method, Fisher's linear discriminant, perceptron algorithm

## 1.3 Least squares for classification

target vector 은 onehot encoding 을 사용하여 표현한다. 그리고 LSM 을 이용하면 각 클래스별 linear function 의 output 을 각 class 에 대한 posteriror 라고 생각할 수도 있다. 그런데 사실 그렇게 보기엔 좀 성능도 안좋고, (0, 1) 밖의 값을 가질수도 있어서 별로다

각 class $C_k$ 를 linear model 로 표현하자 

$$y_k(x) = w_k^T x + w_{k0}$$

위 K 개의 식을 묶어서 하나로 표현할 수 있다.

$$\mathbf{y}(x) = \tilde{W}^T \tilde{\mathbf{x}}$$

training dataset 이 $\lbrace \mathbf{x}_n, $

mean square error 를 다음과 같이 구할 수 있다.

$E_D(\tilde{W}) = \frac{1}{2} Tr \{ (\tilde{X}\tilde{W} - T)^T (\tilde{X}\tilde{W} - T) \}$

미분해서 해를 구하면 다음과 같다

$$\tilde{W} = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T T$$

재밌는 점은 이렇게 구한 $\tilde{W}$ 는 항상 y(x) 의 원소의 합이 1이 된다.

LSM 을 쓰면 해를 closed form 으로 구할 수 있다는 장점을 가지지만 심각한 단점들을 몇가지 지닌다.

먼저 outlier 에 너무 민감하다. 

![](/assets/images/prml2/lsm_outlier.png)

아래와 같이 엉망인 결과를 뽑기도 한다.

![](/assets/images/prml2/lsm_logistic_compare.png)


사실 어쩌면 못하는 것이 당연하다. LSM 를 이용하는 풀이가 target이 gaussian 분포를 이루고 있다는 가정에서 나온 건데 이는 target 값이 binary인 classification 문제에는 적합하지 않다.

## 1.4 Fisher's linear discriminant

dimension reduction 의 관점으로 linear classification model 을 살펴보자. 

먼저 간단하게 생각하기 위해 binary classification 문제라고 가정하겠다

D 차원의 input vector x 를 생각하자. $y = w^T x$. threshold $-w_0$ 두고, $y \ge -w_0$ 면 class 1 에, 아니면 class 2 에 배정한다. D 차원에선 잘 분리 되었던 애들이 1차원에 투영했을땐 겹쳐서 classification 이 잘 안될 수 있지만, $w$ 의 성분을 잘 조절하면 분리를 최대화 하는 $w^T$ 를 선택할 수 있을 것이다.

각 class 의 평균 vector를 구해보자

$m_1 = \frac{1}{N_1}\sum_{n \in C_1} x_n,\ m_2 = \frac{1}{N_2} \sum_{n \in C_2} x_n$

w에 투영했을 때 class 간 분리 정도를 측정하는 가장 쉬운 방법은 투영 된 class 들의 평균이 얼마나 멀리 있는지 보는 것이다. w가 단위 벡터라 하자. 

그런데 이런 방식에는 문제가 있다.

![](/assets/images/prml2/mass.png)

오히려 투영된 평균 vector 의 거리가 작은게 더 잘되는 것을 확인할 수 있다.

이런 문제점을 해결하기 위해 Fisher 가 제안한 아이디어는 w 에 투영된 class 간 평균은 멀게하면서도 분산은 작게 하여 클래스간의 중복을 최소화하자는 것이다.

각 x가 w에 투영된 점을 y라 하자. 이 때 각 class 별로 투영된 data point 들의 분산은 아래와 같다.

$$s_k^2 = \sum_{n \in C_k} (y_n-m_k)^2,\ y_n = w^T x$$

이 때  fisher criterion $J(w)$ 는 다음과 같이 정의된다. 

$$J(w) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}$$

이를 다음과 같이 $w$ 에 대해 종속적인 형태로 변형할 수 있다.

$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

$S_B$ 는 inter-class 공분산 행렬 

$$S_B = (m_2-m_1)(m_2-m_1)^T$$

$S_W$ 는 within class 공분산 행렬이다. 

$$S_W = \sum_{n \in C_1}(x_n - m_1)(x_n - m_1)^T + \sum_{n \in C_2}(x_n - m_2)(x_n - m_2)^T$$

이를 $w$ 에 대해 미분하여 $J(w)$ 를 최대로 만드는 $w$ 를 구하면 다음과 같은 방정식의 해가 됨을 알 수 있다.

$$(w^T S_M w)S_W w = (w^T  S_W w)S_B w$$ 

$w$ 의 크기는 중요하지 않으므로 위 식의 scalar 항인 $(w^T  S_M w)$ 와 $(w^T  S_W w)$ 를 날리면 다음과 같은 식을 얻을 수 있다.

$$w \propto S_W^{-1}(m_2-m_1)$$

## 1.5 Relation to LSM

lsm 과 fissure criterion 의 관계를 알아보자. 언뜻 보면 두 방법은 서로 관계가 전혀 없어 보이지만 사실 fissure criterion 은 lsm 의 special case 로 볼 수 있다.

우리는 lsm 에서 target를 encoding 하는 방법으로 one-hot 을 사용했는데 약간 다른 encoding 방법 ($C_1$ 의 target을 $\frac{N}{N_1}$, $C_2$의 target를 $-\frac{N}{N_2}$) 을 사용하면 LMS 의 해와 fissure 의 해가 동일하다

lsm: $E = \frac{1}{2} \sum_{n=1}^N (w^T x_n + w_0 - t_n)^2$ -> 해 구하면 fissure 와 같음

어떤 insight 가 있는지는 책에서도 안나옴. 그냥 encoding 저렇게 하면 결과가 같다는게 내용 전부임..

## 1.6 multi class 에서 fissure criterion

생략

## 1.7 perceptron algorithm

perceptron 에서는 input vector $x$ 를 fixed nonlinear transformation $\phi$ 를 이용하여 feature vector $\phi(x)$ 로 변형시킨다. 그 후 변환된 feature vector 를 이용하여 다음과 같은 linear model 을 만든다.

$$y(x) = f\left(w^T \phi\left(x\right) \right)$$

$$f(x) = \begin{cases}1&a\ge0 \\ -1& a\lt 0\end{cases}$$

앞에서는 확률적 해석을 위해 target vector 을 $\lbrace 0, 1\rbrace$ 에 포함되게끔 했다. 하지만 perceptron 은 $C_1$는 t=1, $C_2$는 t=-1 로 표현하는 것이 더 좋다.

w는 loss function 최소화하는 방식으로 구할 수 있을 것이다. 우리가 쉽게 생각할 수 있는 loss function 으로는 오분류 sample 개수가 있지만 이는 differentiable 하지 않아서 w 를 구하기엔 좋지 않다.

따라서 여기선 perceptron criterion 이라는 loss function 소개한다.

class $C_1$ 에 속한 input vector $x$ 는 $w^T \phi(x) \gt 0$, class $C_2$ 에 속한 input vector $x$ 는 $w^T \phi(x) \lt 0$ 가 되어야 하는데 이는 아래와 같은 식 하나로 정리할 수 있다.

$$w^T \phi(x_n) t_n \gt 0$$

perceptron criterion 은 옳게 분류된 sample 에서는 0 의 loss 를 주고 오분류한 sample 들에만 위의 식을 최대화 하고자 한다. 따라서 perceptron criterion 은 다음과 같다.

$E_p(w) = - \sum_{n \in M} w^T \phi_n t_n$, $M$은 오분류된 sample들

perceptron algorithm 은 확률적 해석이 가능한 출력값을 내지 않는다. K > 2 인 문제에 대해 일반화가 되지 않는다. 그리고 무엇보다 fixed basis function 의 linear combination 으로 이루어져있다는게 가장 큰 한계점이다. 이 한계점에 대한 논의는 다른 책을 보자(...)

# 2. Probabilistic Generative models

이제 classification 문제를 probabilistic view 로 다뤄보자. data 의 분포에 대한 단순한 가정으로부터 시작하여 linear decision boundary 를 가지는 model 를 유도해 낼 것이다.

우리는 generative 방식을 이용할 것이다. $p(x \vert C_k)$, $p(C_k)$ 를 model 하고 posterior 계산해낸다.

먼저 문제를 간단하게 다루기 위해 class 가 2개라고 가정하자. class 1 에 대한 posterior 를 다음과 같이 적을 수 있다.

$$\begin{matrix}
p(C_1 \vert x) &=& \frac{p(x \vert C_1) p(C_1)}{p(x \vert C_1) p(C_1)+p(x \vert C_2) p(C_2)} \\
&=& \frac{1}{1+\exp(-a)} \\
&=& \sigma(a)
\end{matrix}$$
  
여기서 

$$a = \log \frac{p(x \vert C_1) p (C_1)}{p(x \vert C_2) p (C_2)}$$

이다.

sigmoid 를 만들었다! 잠시 후 (2.1)에 class 별 contitional 이 gaussian 이란 가정하에 이 sigmoid 의 내부항이 linear 가 됨을 보인다.

이제 class k 개일 떄를 다뤄보자.

$$\begin{matrix}
p(C_k \vert x) &=& \frac{p(x \vert C_k)p(C_k)}{\sum_j p(x \vert C_j)p(C_j)} \\
&=& \frac{\exp(a_k)}{\sum_j \exp(a_j)}
\end{matrix}$$

여기서

$$a_k = \log (p(x \vert C_k) p(C_k))$$

softmax 를 만들었다! 잠시 후 (2.1)에 class 별 contitional 이 gaussian 이란 가정하에 이 softmax 의 내부항이 linear 가 됨을 보인다.

## 2.1 Continuous inputs

class 별 conditional probability $p(x \vert C_k)$ 가 gaussian 이라 가정하자. 여기에 추가로 모든 class가 같은 공분산 행렬 공유한다고 가정한다

$$p(x \vert C_k) = \frac{1}{(2\pi)^{D / 2}} \frac{1}{\left\vert \Sigma \right\vert^{1/2}} \exp \{ -\frac{1}{2} (x- \mu_k)^T \Sigma^{-1} (x- \mu_k) \}$$

class 가 2 개일 때 구했던 sigmoid 에 위의 식을 대입하면 다음과 같은 식을 얻을 수 있다.

$$p(C_1 \vert x) = \sigma(w^T x + w_0)$$

**눈여결 볼 점은 sigmoid 안의 값($=a(x)$)이 x에 대한 선형함수라는 것이다**. 따라서 decision boundary 는 linear 가 된다.

class 가 K 개일 때는

$$a_k(x) = w_k^T x + w_{k0}$$ 

**여전히 x 에 대한 선형함수가 됨을 확인할 수 있다.**

공분산이 공유되지 않으면 decision boundary 는 quadratic 이 된다.

## 2.2 MLE

$p(x \vert C_k)$ 형태를 명시하고 mle 를 쓰면 $p(x \vert C_k)$의 parameter들과 $p(C_k)$ 를 구할 수 있다.

class 가 2개, $p(C_1) = \pi$ 라 하자. 간단하게 다루기 위해 공분산 행렬을 공유한다 하자.

$p(x_n, C_1) = p(C_1)p(x_n \vert C_1) = \pi \mathcal{N} (x_n \vert \mu_1, \Sigma)$

$p(x_n, C_2) = p(C_2)p(x_n \vert C_2) = (1-\pi) \mathcal{N} (x_n \vert \mu_2, \Sigma)$

이제 likelihood 를 구하면 아래와 같다.

$$p(t, x \vert \pi, \mu_1, \mu_2, \Sigma) = \prod_{n=1}^N \left[ \pi \mathcal{N}(x_n \vert \mu_1, \Sigma) \right]^{t_n} \left[ (1-\pi) \mathcal{N} (x_n \vert \mu_2, \Sigma) \right]^{1-t_n}$$

이를 최대로 만드는 $\pi$ 를 구해보면 다음과 같다.

$$\pi = \frac{N_1}{N_1+N_2}$$

최대로 만드는 $\mu_1$ 값도 구해보면 아래와 같이 얻어진다.

$$\mu_1 = \frac{1}{N_1} \sum_{n=1}^N t_n x_n$$

$$\mu_2 = \frac{1}{N_2} \sum_{n=1}^N (1-t_n)x_n$$

공분산 행렬 $\Sigma_{MLE}$ 도 구하면 아래와 같다. 얘는 계산 과정이 좀 복잡하다.

$$\Sigma = S_1 = \frac{1}{N_1} \sum_{n \in C_1} (x_n - \mu_1)(x_n - \mu_1)^T$$

k 개 class 확장도 쉽게 됨

## 2.3 Discrete features

input 이 discrete 할때를 고려해보자. 논의를 간단히 하기 위해 $x_i \in \lbrace 0, 1 \rbrace$ 라고 하자. 그리고 각 input feature 가 모두 $C_k$ 에 대해 조건부 독립이라고 가정하자. 이와 같은 가정은 Naive bayes 가정이라 불린다. 이 가정 하에 class 에 대한 조건부 분포는 다음과 같다.

$$p(\mathbf{x} \vert C_k) = \prod_{i=1}^D \mu_{ki}^{x_i} (1-\mu_{ki})^{1-x_i}$$

이를 아까 구했던 $a_k = \log (p(x \vert C_k) p(C_k))$ 에 대입하면 다음과 같은 식이 구해진다.

$$a_k(\mathbf{x}) = \sum_{i=1}^D \lbrace x_i \log \mu_{ki} + (1-x_i) \log (1-\mu_{ki}) \rbrace + \log p(C_k)$$

이는 또 input vector $x_i$ 에 대한 linear function 임을 확인 할 수 있다.

## 2.4 Exponential Family

앞에서 우리는 Gaussian 을 따르는 input 의 class 에 대한 posterior 가 sigmoid(K=2), softmax(K>2) 을 activation 으로 가지는 linear model 로 주어짐을 확인했다.

이제 class conditional probability $p(x \vert C_k)$ 를 단순 Gaussian 을 넘어 exponential family 라고 했을때의 결과들을 도출해보자

$$p(x \vert \lambda_k) = h(x) g(\lambda_k) \exp \lbrace \lambda_k^T u(x) \rbrace$$

$u(x) = x$ 인 상황에서만 고려하자.

(과정 생략)

결과론 적으로 또다시 k=2 일땐 sigmoid 안의 $a(x)$ 가 x 에 대한 선형 함수임을 알 수 있고 k>2 일땐 softmax 안의 a가 x에 대한 선형 함수임을 얻는다.

# 3. Probabilistic Discriminative Models

앞에서 우리는 다양한 종류의 class conditional probability $p(x \vert C_k)$ 에 대해 x의 class k 에 대한 posterior 를 linear 에 activation 으로 sigmoid 혹은 softmax 를 가지는 model 로 표현할 수 있음을 살펴봤다. 또한 conditional 이 gaussian 을 따르는 상황에서는 MLE 를 이용하여 직접 posterior 를 구해보기도 했다.

classification 을 하는 다른 방법이 또 있다. 바로 $p(C_k \vert x)$ 를 직접적으로 구하는 방법이다. 이는 generative model 이 conditional probability 의 실제 분포를 잘 근사하는데 실패했을 때 잘 동작한다.

## 3.1 fixed basis function

basis function 을 이용하면 원래 input space 에서는 선형으로 구별되지 않던 feature 들을 선형으로 구별되는 새로운 space 로 옮길 수 있다. 사실 basis function 을 사용하더라도 완벽하게 구별시킬수 없는 상황은 있으나 그래도 모델링하는 과정이 쉬워진다.

## 3.2 logistic regression (binary classification)

앞서 2.4절에서 우리는 여러 종류의 클래스 조건 분포에서 class C1 에 대한 posterior 를 아래와 같이 linear function 에 대한 sigmoid 로 표현 가능함을 증명했었다. (K=2)

$$p(C_1 \vert \phi) = y(\phi) = \sigma (w^T \phi)$$

위와 같은 model 은 통계학에서 logistric regression 이라 불린다. (regression 이긴 한데 classification 을 위한 regression 이다)

M 차원 feature space $\Phi$ 에 대해 위의 모델은 M 개의 adjustable parameter 들을 가지고 있다. 2절에서 가우시안을 이용하여 모델링 했던때를 떠올려 보면 그때는 평균값에 대해 2M 개의, 공분산 행렬에 대해서는 $M(M+1)/2$ 개의 parameter 를 가졌던 것에 비하면 훨씬 적은 숫자이다.

dataset $\lbrace \phi_n, t_n \rbrace$ 에 대해 likelihood 를 계산 하면 다음과 같다.

$$p(\mathbf{t} \vert \mathbf{w}) = \prod_{n=1}^{N} y_n^{t_n} \lbrace 1- y_n\rbrace^{1-t_n}, y_n = p(C_1 \vert \phi_n)$$

log likelihood 를 구해보면 매우 친숙한 꼴이 나옴을 확인할 수 있다.

$$E(\mathbf{w}) = -\log p(\mathbf{t} \vert \mathbf{w}) = -\sum_{n=1}^N  \lbrace t_n \log y_n + (1-t_n) \log (1-y_n) \rbrace$$

위 식은 **cross entropy** error function 이라 불린다.

## 3.4 Multiclass logistic regression

앞서 2.4절에서 우리는 여러 종류의 클래스 조건 분포에서 class $C_k$ 에 대한 posterior 를 아래와 같이 linear function 에 대한 softmax 로 표현 가능함을 증명했었다. (K>2)

$$p(C_k \vert \phi) = y_k(\phi) = \frac{\exp(a_k)}{\sum \exp(a_j)}, a_k = \mathbf{w}_k^T \phi$$

likelihood 는 다음과 같다

$$p(\mathbf{T} \vert \mathbf{w}_1, \dots, \mathbf{w}_K) = \prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_nk}$$

log likelihood 를 구하면 친숙한 꼴이 나온다.

$$E(\mathbf{w}_1, \dots, \mathbf{w}_K) = -\log p(\mathbf{T} \vert \mathbf{w}_1, \dots, \mathbf{w}_K) = -\sum_{n=1}^N \sum_{k=1}^K t_{nk} \log y_{nk}$$

이는 multiclass 에서의 cross entropy error function 이라 부른다.

## 3.5 Probit regression

지금까진 클래스 조건 분포가 지수족으로 표현 가능한 케이스에 대해서만 이들이 logistic 혹은 softmax 로서 표현됨을 살펴보았다. 하지만 세상엔 지수족으로 표현 가능한 케이스만 존재한는 것이 아니므로 다른 경우들도 따져보도록 하자.

K=2 인 상황을 따져보자. general 한 형태의 linear model +activation 은 다음과 같다.

$$p(t=1 \vert a) = f(a), a = \mathbf{w}^T \phi$$


