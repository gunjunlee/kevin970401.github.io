---
layout: post
title: "PRML Ch.6 - Kernel methods"
categories: PRML
author: lee gunjun
---

# intro

ch3, 4 에서 regression 과 classification 에 대한 linear parametric model 을 살펴봤다. 이 때는 train dataset 은 매개변수의 point estimate 를 구하고 버려지거나, 그 posterior 를 구한 뒤에 버려졌다. 즉 prediction 에는 더 이상 train dataset 이 필요하지 않았다.

이와는 반대로 training dataset 의 전부, 혹은 일부를 prediction 단계에서 사용하는 방법이 존재한다. k-NN 과 같은 기법이 그들 중 하나의 예시이다. 

많은 linear parametric model 들은 equivalend dual representation 을 가진다. 이 dual presentation 는 prediction 을 각 training data points 에 대해 얻어지는 kernel function 들의 선형 결합으로 계산한다.

kernel function 은 아래와 같은 형태를 가진다.

$$k(x, x') = \phi(x)^T \phi(x)$$

kernel function 은 대칭적이다. 그리고 feature space 상의 내적으로 정의되어진다. 간단한 예시로 $\phi(x) = x$ 인 경우 $k(x, x')=x^T x'$ 이며 이는 linear kernel 이라 불린다.

kernel function 에는 여러 종류가 있다.

# 1. Dual representation

아래의 regularized sum-of-squares error function 의 dual representation 을 찾아보자.

$$J(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \lbrace \mathbf{w}^T \phi(\mathbf{x}_n) - t_n \rbrace + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}$$

미분을 하여 최솟값을 만드는 w 에 대한 식을 구하면

$$w = \frac{1}{\lambda} \sum_{n=1}^N \lbrace \mathbf{w}^T \phi (\mathbf{x}_n) -t_n\rbrace \phi(\mathbf{x_n}) = \sum_{n=1}^N a_n \phi(\mathbf{x}_n) = \Phi^T \mathbf{a}$$

여기서 $Phi$ 는 n 번째 row 가 $\phi(x_n)^T$ 인 desing matrix, $\mathbf{a} = (a_1, \dots, a_N)^T$ 이고, 

$$a_n = -\frac{1}{\lambda} \lbrace \mathbf{w}^T \phi(\mathbf{x}_n) - t_n \rbrace$$

이다.

이제 다시 $J(w)$ 의 식에서 $\mathbf{w} = \Phi^T \mathbf{a}$ 를 대입하여 아래의 식을 구할 수 있다.

$$J(a) = \frac{1}{2} \mathbf{a}^T K K \mathbf{a} - \mathbf{a} K \mathbf{t} + \frac{1}{2} \mathbf{t}^T \mathbf{t} + \frac{\lambda}{2} \mathbf{a}^T K \mathbf{a}$$

여기서 우리는 gram matrix $K=\Phi \Phi^T$ 를 정의했다. $K$ 의 원소는 다음과 같다.

$$K_{nm} = \phi(\mathbf{x}_n)^T \phi(\mathbf{x}_m) = k(x_n, x_m)$$

$J(a)$ 를 최대로 만드는 a 는 다음과 같이 얻어진다

$$\mathbf{a} = (K + \lambda I_N)^{-1} \mathbf{t}$$

이를 linear regression 식에 대입하면 새 input 에 대한 prediction 은 다음과 같이 구해진다.

$$y(\mathbf{x}) = \mathbf{w}^T \phi(\mathbf{x}) = \mathbf{a}^T \Phi \phi(\mathbf{x}) = \mathbf{k}(\mathbf{x})^T (K + \lambda I_N)^{-1} \mathbf{t}$$

vector $\mathbf{k}(\mathbf{x})$ 를 새로 정의했다. $\mathbf{k}_n(\mathbf{x}) = k(\mathbf{x}_n, \mathbf{x})$ 이다.

즉 prediction training set 의 data point 들과 kernel function 을 취한 값들의 linear combination 으로 얻을 수 있다.

# 2. 