---
layout: post
title: "PRML Ch.1 - Introduction"
categories: PRML
author: lee gunjun
---

prml 을 공부한 사람을 위한 글입니다.  
공부하실 분들에겐 더 좋은 블로그가 있으니 추천드립니다. [norman3 님 블로그](http://norman3.github.io/prml/)

무엇을 알아갈 것인가

1. frequentist 와 bayesian
2. iid
3. MLE
4. 1D gaussian 에서의 MLE
5. gaussian MLE 에서 LMS 유도
6. MAP
7. gaussian MAP 에서 l2 norm 유도
8. classification 에서의 결정 문제 3가지
9. regression 에서의 결정 문제 3가지
10. entropy
11. KL divergence

시작하자.

----

베이지안 정리: $p(w \vert D) = \frac{p(D \vert w)p(w)}{p(D)}$

likelihood: $p(D \vert w)$
prior: $p(w)$
post: $p(w \vert D)$

* frequentist: w는 고정된 값.
* bayesian: w의 불확실성 구함.

frequentist 는 estimator 로 likelihood 를 쓸때가 많음.  
observed data points D 을 가지고 likelihood 가 최대(maximum likelihood)가 되는 w를 찾음.

i.i.d(independent and identically distributed): 데이터가 각각 동일한 분포에서 독립적으로 뽑힘.

maximum likelihood 를 이용하여 1-dim gaussian distrib 에서 뽑힌 N개의 data $(x_1, \cdots, x_n)$ 을 가지고 매개변수 $\mu \text{와} \sigma$를 구해보자.

$\log p(\mathbf{x} \vert \mu, \sigma) = -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2} \log \sigma^2-\frac{N}{2} \log (2\pi)$

이를 $\mu \text{와} \sigma$에 대해 미분하여 최대가 되는 매개변수 값을 구하면

$\mu^{MLE} = \frac{1}{N}\sum_{n=1}^N x_n$  
${\sigma^2}^{MLE} = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{MLE})^2$

근데 expectation 을 계산해보면

$\mathbb{E}\left[\mu^{MLE}\right] = \mu$  
$\mathbb{E}\left[{\sigma^2}^{MLE}\right] = \frac{N-1}{N}\sigma^2$

평균은 unbiased 라 괜찮은데, 분산은 과소평가되는 biased-estimator 임을 확인할 수 있다.  
그래서 우린 종종 unbiased-estimator ${\tilde{\sigma}^{2}}^{MLE} = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu^{MLE})^2$ 를 사용한다.

이제 mle 에서 lms(least mean square) 를 유도해보자.

training set: $\mathbf{x} = (x_1, \cdots, x_n)^T, \mathbf{t} = (t_1, \cdots, t_N)^T$ 이고 x 값이 $y(x, \mathbf(w))$ 값을 평균으로 가지는 가우시안 분포 를 가진다고 가정하자.  
$p(t \vert x, \mathbf{w}, \beta) = \mathcal{N} (t \vert y(x, \mathbf{w}), \beta^{-1})$ 가 된다.

training set 추출 과정이 i.i.d 라 하면  
likelihood 는 $p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) = \Pi_{n=1}^N \mathcal{N} (t_n \vert y(x_n, \mathbf{w}), \beta^{-1})$ 가 된다.  
로그 가능도 함수를 구하면  
log likelihood = $\log p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) = -\frac{\beta}{2}\sum_{n=1}^{N} \left\{ y(x_n, \mathbf{w}) - t_n \right\}^2+\frac{N}{2}\log\beta - \frac{N}{2} \log(2\pi)$  

**$w$에 대해 가능도함수의 최댓값을 구하는 것은 제곱합 오차 함수의 최댓값을 구하는 것이 된다.**

$w^{MLE}$와 $\beta^{MLE}$ 를 구했으면 이제 prediction이 가능해진다.  
$p(t \vert x, \mathbf{w}^{MLE}, \beta^{MLE}) = \mathcal{N}(t \vert y(x, \mathbf{w}^{MLE}), {\beta^{MLE}}^{-1})$

여기까진 frequentist 관점이었다.  
bayesian 관점으로 생각해보자.

$w$에 대한 prior 를 먼저 가정해야한다. 이 prior 의 토대 안에서 w의 distribution을 구할 것이다.

$p(\mathbf{w} \vert \alpha) = \mathcal{N} (\mathbf{w} \vert \alpha^{-1} \mathbf{I}), \text{w는 M-dim}$ 라 하자.

베이지안 정리에 의해 w의 posterior 는 prior 와 likelihood 의 곱에 비례한다.  
$p(\mathbf{w} \vert \mathbf{x}, \mathbf{t}, \alpha,\beta) \propto p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} \vert \alpha)$  
bayesian 에서는 posterior 에서 가장 높은 w를 찾는 방식으로 w를 결정할 수 있다. 이를 MAP(maximum posterior) 라 한다.  
식을 대입해서 실제로 구해볼 수도 있으나 귀찮으니 생략하고 결과만 적으면  
$\frac{\beta}{2}\sum_{n=1}^N \left\{y(x_n, \mathbf{w}) - t_n\right\}^2 + \frac{\alpha}{2}\mathbf{w}^T\mathbf{w}$ 을 최소화 하는 w를 구하는 것과 동일하다.  

근데 여기서 여전히 w에 대한 point estimation 을 하고 있기 때문에 완벽한 bayesian 이라 하기 어렵다. 완벽한 bayesian 이 되기 위해 적분을 하자.

다시 한번 상기해보면 결국 하려는 건 training set 를 활용하여 새로운 입력 x 가 들어왔을 때 결과값 t의 분포를 구하는 것을 하고 싶은 것이다. 이를 식으로 써보면 $p(t \vert x, \mathbf{x}, \mathbf{t})$ 이 된다. ($\alpha, \beta$ 고정)

prediction은 $p(t \vert x, \mathbf{x}, \mathbf{t}) = \int p(t \vert x, \mathbf{w})p(w \vert \mathbf{x}, \mathbf{t}) d\mathbf{w}$ 이 된다. 적분 하면 됨. 과정, 결과 복잡하니 생략.

-----
 
복잡한 모델에서 overfitting 이 일어나는거 때문에 penalty 를 추가해서 model 를 고르곤 함. 그중 하나로 AIC(akaike information criterion) 가 있음  
AIC = $log p(D \vert \mathbf{w}^{MLE}) - M\text{, M은 모델의 매개변수 수}$

----

차원의 저주

data dim 이 커지면 training set 는 기하급수적으로 커져야함

----

decision theory

불확실성이 있을 때 의사 결정을 어떻게 할까.  
확률들이 주어진 상태에서 어떻게 해야 최적의 결정을 내릴 수 있을지 선택하는 것이다.
아마 posterior 가 가장 큰 결과값을 선택 할 것이다

loss function 과 decision theory 은 깊은 관련이 있다. 그리고 우리가 흔히 머신러닝에서 사후확률을 decision 할 때 쓰는데 그에 대한 이유도 나온다.

입력변수를 x, 그에 대한 target vector 를 t 라 하자. inference 를 p(x, t) 를 찾아내는 것이라 하자.

우리는 각각의 x를 가능한 클래스 $C_k$(k=1, ..., c) 중 하나에 포함시키는 결정을 할 것이다.  
그러한 결정은 input space 를 decision region 이라 불리는 구역 $R_k$ 로 나누게 할 것이다. ($R_i \text{에 속하는 input 은} C_k\text{에 속함}$)  

binary classification 같은 경우엔 k=1, 2이고 실수가 발생할 확률은  
$p(mistake) = p(x \in R_1, C_2) + p(x \in R_2, C_1) = \int_{R_1} p(x, C_2)dx + \int_{R_2} p(x, C_1)dx$ 이다.  

우리는 아마 mistake 를 가장 작게하는 decision 을 해야 할 것이다. 즉 $p(x, C1) > p(x, C2)$ 인 경우에는 $x$를 $C_1$ 에 속하게 해야한다. 그런데 위 식은 $p(C1 \vert x)p(x) > p(C2 \vert | x)p(x)$ 와 같고 이것은 posterior 가 큰 클래스를 선택하는 것이 된다. multi class 로의 확장은 쉬우니 생략.

loss function 의 개념이 나온다.  
위처럼 mistake 를 적게 하는 decision 뿐 아니라 가령 $p(x \in R_1, C2)$ 만 최소가 되면 되는 decision 이 있을 수 있다. 이를 결정하는게 loss function 이다.

어느 구역에도 넣기 애매할 때는 reject 하는 option 을 택하기도 한다.

우리는 지금까지 1. inference stage 를 통해 posterior 를 추론하고 2. decision stage 를 통해 최종 target 을 예측하는 방법을 살펴 봤다. 이외에 다른 방법이 있을까?  
결정문제를 푸는데는 세가지 다른 방법이 있다.
1. generative model
1. discriminative model
1. 확률 계산 안하고 그냥 input 넣으면 바로 class 나오는 model ~~(뭐라 불러야 해 이걸)~~

각각에 대한 설명

1. generative model  
각각의 class $C_k$ 에 대해 $p(x \vert C_k)$ 를 inference stage 로 풀어낸다. 그리고 각 class 에 대한 $p(C_k)$ 로 따로 구한다. 이를 이용하여 posterior 를 계산한다. 이렇게 posterior를 알아낸후에는 decision theory 를 이용하여 class 를 예측한다.  
이는 generative model 이라 불리는데 위에서 $p(x)$ 를 구해서 input space 의 합성 data를 생성할 수 있기 때문이다.
2. discriminative model  
위에서 열심히 했던 것이다. posterior 를 inference stage 로 추론하고 decision theory 이용해서 결정. 이땐 $p(C_k)$ 등의 분포는 관심대상이 아니다.
3. 설명 생략

당연히 discriminative model 가 generative model 보다 class 예측하는 데에선 효과적이다.

3의 방법을 쓰면 posterior 를 계산하지 않기 때문에 risk 관리가 힘들어 금융에서 사용하기 안 좋다.

input 이 여러개면 어떡할까? 간단히 독립이라 가정하는 방법이 있다.(naive bayes model)  
$p(x1, x2 \vert C_k) = p(x1 \vert C_k) p(x2 \vert C_k)$  
사후확률은 $p(C_k \vert x1, x2) \propto \frac{p(C_k \vert x1)p(C_k \vert x2)}{p(C_k)}$ 가 된다.

지금까진 classification 에서 decision theory 를 함.  
이제 regression 에서 decision theory 해보자.

regression function: $f(x)= \mathbb{E}\left[t \vert x \right]$  
regression 에서 일반적으로 쓰는 loss function 인 lms를 가장 적게 하는 f(x) 값 구할 때 나옴.  
$\mathbb{E}\left[ L\right] = \int \int L(t, y(t))p(x, t)dxdt = \int \int (t-y(t))^2 p(x, t)dxdt$ $y(t)$로 양변 미분하면$f(x)= \mathbb{E}\left[t \vert x \right]$ 나옴  

regression 을 푸는 방법도 여러가지 있음  
1. $p(x, t)$ 풀고 이를 통해 $p(t \vert x)$ 구하고 $\mathbb{E}\left[t \vert x \right]$ 계산
2. $p(t \vert x)$ 구하고 $\mathbb{E}\left[t \vert x \right]$ 계산
3. $y(x)$ 바로 계산

l2 loss 를 사용했는데 이는 multi-modal distribution 에는 적절치 않다.   
l2 loss 를 generalize 한 loss 가 Minkowski loss 다  
$\mathbb{E}\left[L_q\right] = \int \int \left\vert y(x) - t \right\vert^q p(x, t) dx dt$  
q = 2 이면 조건부 평균, q = 1이면 조건부 중앙값, q = 0이면 조건부 최빈값일 때 최소가 된다.

---- 

**information theory**

정보량 $h(x)$ 은 $p(x)$ 에 단조 감소이고, 서로 연관되지 않은 두 사건 x와 y에 대해 $h(x, y) = h(x) + h(y)$ 이다.  
$h(x) = -\log_2p(x)$ 가 이를 만족한다.  
**엔트로피** := **정보량의 평균치** = $H\left[x\right] = - \sum_x p(x) \log_2 p(x)$ 이다.  
예시로 class 가 n개고 균등 분포인 걸 생각하면 $H\left[x\right] = -n * \frac{1}{n} \log_2 \frac{1}{n} = \log_2 n$

shannon 에 의하면 엔트로피는 확률변수의 상태를 전송하기 위해 필요한 비트 숫자의 하한선이다.(noiseless coding theorem)

앞으론 $\log_2$ 대신 $\log$ 사용

conditional entropy:

$$\begin{matrix}
H[x, y] &=& -\int \log p(x, y) p(x, y) dxdy\\
& =& -\int \log \left[p(y \vert x) p(x)\right] p(x, y) dx dy\\
& =& -\int \log p(y \vert x) p(x, y)dydx - \int \log p(x) p(x) dx\\
& =& H[y \vert x] + H[x]
\end{matrix} $$

**KL divergence (= relative entropy)**  

$$\begin{matrix}
KL(p \parallel q) &=& -\int p(x) \log q(x) dx - \left(-\int p(x) \log p(x) dx\right)\\
&=&-\int p(x) \log \frac{q(x)}{p(x)} dx
\end{matrix}$$