---
layout: post
title: "PRML Ch.1 - Introduction"
categories: PRML
author: lee gunjun
---

prml 잘 정리된 블로그 추천: [norman3 님 블로그](http://norman3.github.io/prml/)

무엇을 알아갈 것인가

1. frequentist 와 bayesian
2. iid
3. MLE
4. 1D gaussian 에서의 MLE
5. gaussian MLE 에서 LMS 유도
6. MAP
7. gaussian MAP 에서 l2 norm 유도
8. classification 에서의 결정 문제 3가지
9. regression 에서의 결정 문제 3가지
10. entropy
11. KL divergence

시작하자.

# 1.2 Probability

베이지안 정리: $p(w \vert D) = \frac{p(D \vert w)p(w)}{p(D)}$

likelihood: $p(D \vert w)$
prior: $p(w)$
post: $p(w \vert D)$

* frequentist: w는 고정된 값.
* bayesian: w의 불확실성 구함.

frequentist 는 estimator 로 likelihood 를 쓸때가 많음.  
observed data points D 을 가지고 likelihood 가 최대(maximum likelihood)가 되는 w를 찾음.

i.i.d(independent and identically distributed): 데이터가 각각 동일한 분포에서 독립적으로 뽑힘.

maximum likelihood 를 이용하여 1-dim gaussian distrib 에서 뽑힌 N개의 data $(x_1, \cdots, x_n)$ 을 가지고 매개변수 $\mu \text{와} \sigma$를 구해보자.

$\log p(\mathbf{x} \vert \mu, \sigma) = -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2-\frac{N}{2} \log \sigma^2-\frac{N}{2} \log (2\pi)$

이를 $\mu \text{와} \sigma$에 대해 미분하여 최대가 되는 매개변수 값을 구하면

$\mu^{MLE} = \frac{1}{N}\sum_{n=1}^N x_n$  
${\sigma^2}^{MLE} = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu^{MLE})^2$

근데 expectation 을 계산해보면

$\mathbb{E}\left[\mu^{MLE}\right] = \mu$  
$\mathbb{E}\left[{\sigma^2}^{MLE}\right] = \frac{N-1}{N}\sigma^2$

평균은 unbiased 라 괜찮은데, 분산은 과소평가되는 biased-estimator 임을 확인할 수 있다.  
그래서 우린 종종 unbiased-estimator ${\tilde{\sigma}^{MLE}}^{2} = \frac{1}{N-1}\sum_{n=1}^{N}(x_n-\mu^{MLE})^2$ 를 사용한다.

이제 mle 에서 lms(least mean square) 를 유도해보자.

training set: $\mathbf{x} = (x_1, \cdots, x_n)^T, \mathbf{t} = (t_1, \cdots, t_N)^T$ 이고 x 값이 $y(x, \mathbf(w))$ 값을 평균으로 가지는 가우시안 분포 를 가진다고 가정하자.  
$p(t \vert x, \mathbf{w}, \beta) = \mathcal{N} (t \vert y(x, \mathbf{w}), \beta^{-1})$ 가 된다.

training set 추출 과정이 i.i.d 라 하면  
likelihood 는 $p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) = \Pi_{n=1}^N \mathcal{N} (t_n \vert y(x_n, \mathbf{w}), \beta^{-1})$ 가 된다.  
로그 가능도 함수를 구하면  
log likelihood = $\log p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) = -\frac{\beta}{2}\sum_{n=1}^{N} \left[ y(x_n, \mathbf{w}) - t_n \right]^2+\frac{N}{2}\log\beta - \frac{N}{2} \log(2\pi)$  

**$w$에 대해 가능도함수의 최댓값을 구하는 것은 제곱합 오차 함수의 최솟값을 구하는 것이 된다.**

$w^{MLE}$와 $\beta^{MLE}$ 를 구했으면 이제 prediction이 가능해진다.  
$p(t \vert x, \mathbf{w}^{MLE}, \beta^{MLE}) = \mathcal{N}(t \vert y(x, \mathbf{w}^{MLE}), {\beta^{MLE}}^{-1})$

여기까진 frequentist 관점이었다. 여기에 약간의 bayesian 관점을 더해보자. 그렇다고 아직 bayesian 을 다룰 건 아님.

bayesian 에서는 $w$에 대한 prior 를 먼저 가정해야한다. 자신의 *믿음*에 따라 prior 를 세우면 된다. 이 prior 의 토대 안에서 w의 distribution을 구할 것이다.

$p(\mathbf{w} \vert \alpha) = \mathcal{N} (\mathbf{w} \vert \alpha^{-1} \mathbf{I}), \text{w는 M-dim}$ 라 하자. 이때 $\alpha$ 를 hyper-parameter 라 한다 (remind: w 는 parameter 라 함). hyperparameter 는 보통 고정된 값으로 취급하며 이를 통해 parameter 의 확률분포가 결정된다. 참고로 hyperparameter 또한 확률변수로 보고 hyper-hyperparameter 를 고려하는 경우도 있다. 이러한 것을 recursive bayesian estimation 이라 한다.

베이지안 정리에 의해 w의 posterior 는 prior 와 likelihood 의 곱에 비례한다.  
$p(\mathbf{w} \vert \mathbf{x}, \mathbf{t}, \alpha,\beta) \propto p(\mathbf{t} \vert \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} \vert \alpha)$  
이때 posterior 에서 가장 높은 w를 찾는 방식으로 w를 결정할 수 있다. 이를 MAP(maximum posterior) 라 한다.  
식을 대입해서 실제로 구해볼 수도 있으나 귀찮으니 생략하고 결과만 적으면  
$\frac{\beta}{2}\sum_{n=1}^N \{y(x_n, \mathbf{w}) - t_n\}^2 + \frac{\alpha}{2}\mathbf{w}^T\mathbf{w}$ 을 최소화 하는 w를 구하는 것과 동일하다.  

근데 여기서 여전히 w에 대한 point estimation 을 하고 있기 때문에 완벽한 bayesian 이라 하기 어렵다. 완벽한 bayesian 이 되기 위해 적분을 하자.

다시 한번 상기해보면 결국 하려는 건 training set 를 활용하여 새로운 입력 x 가 들어왔을 때 결과값 t의 분포를 구하는 것을 하고 싶은 것이다. 이를 식으로 써보면 $p(t \vert x, \mathbf{x}, \mathbf{t})$ 이 된다. ($\alpha, \beta$ 고정)

prediction은 $p(t \vert x, \mathbf{x}, \mathbf{t}) = \int p(t \vert x, \mathbf{w})p(w \vert \mathbf{x}, \mathbf{t}) d\mathbf{w}$ 이 된다. 적분 하면 됨. 과정, 결과 복잡하니 생략.

참고로 위의 prediction 또한 gaussian distribution 으로 분석할 수 있다.

$$p(t \vert x, \mathbf{x}, \mathbf{t}) = \mathcal{N}(t \vert m(x), s^2(x))$$


# 1.3 Model Selection
 
복잡한 모델에서 overfitting 이 일어나는거 때문에 penalty 를 추가해서 model 를 고르곤 함. 그중 하나로 AIC(akaike information criterion) 가 있음  
AIC = $log p(D \vert \mathbf{w}^{MLE}) - M\text{, M은 모델의 매개변수 수}$

# 1.4 Curse of Dimensionality

차원의 저주

data dim 이 커지면 training set 는 기하급수적으로 커져야함

# 1.5 Decision Theory

불확실성이 있을 때 의사 결정을 어떻게 할까.  
확률변수의 분포가 주어진 상태에서 어떤 값을 선택해야 최적의 결정을 내릴 수 있을지에 대한 고민이다.
아마 posterior 가 가장 큰 결과값을 선택 할 것이다

loss function 과 decision theory 은 깊은 관련이 있다. 그리고 우리가 흔히 머신러닝에서 사후확률을 decision 할 때 쓰는데 그에 대한 이유도 앞으로 다룰 것이다.

입력변수를 x, 그에 대한 target vector 를 t 라 하자. inference 를 p(x, t) 를 찾아내는 것이라 하자.

우리는 각각의 x를 가능한 클래스 $C_k$(k=1, ..., c) 중 하나에 포함시키는 결정을 할 것이다.  
그러한 결정은 input space 를 decision region 이라 불리는 구역 $R_k$ 로 나누게 할 것이다. ($R_i \text{에 속하는 input 은} C_i\text{라고 결정}$)  

binary classification 같은 경우엔 k=1, 2이고 잘못된 결정을 내리는 실수가 발생할 확률은  
$p(mistake) = p(x \in R_1, C_2) + p(x \in R_2, C_1) = \int_{R_1} p(x, C_2)dx + \int_{R_2} p(x, C_1)dx$ 이다.  

우리는 아마 mistake 를 가장 작게하는 decision 을 해야 할 것이다. 즉 $p(x, C1) > p(x, C2)$ 인 경우에는 $x$ 를 $C_1$ 에 속하게 해야한다. 그런데 위 식은 $p(C1 \vert x)p(x) > p(C2 \vert x)p(x)$ 와 같다. 즉 mistake 를 줄이기 위해선 posterior 가 큰 클래스를 선택해야한다는 결론이 도출된다.

위는 binary class 에서 살펴본것이지만 multi class 에서도 같은 결과를 얻을 수 있다. 이는 binary 와 별다를 것 없이 간단하므로 생략한다.

하지만 현실은 위처럼 단지 mistake 만을 작게 만들면 되는 decision 뿐만이 있는 것이 아니다. 예를 들어 암을 판별하는 task 를 들어보자. 암이 아닌 것을 암이라 하는 것은 용서되겠지만 암인 것을 암이 아니라 하는 것은 용서못할 실수가 될 것이다. 이렇게 단순 mistake 가 아닌 다른 목적을 가지는 decision 에 대해서도 우리는 생각해봐야 한다. 이러한 decision 결정하기 위해 우리는 loss function 이란 개념을 도입한다. decision 은 바로 이 loss function 을 최소화 하는 방향으로 내려질 것이다.

참고로 어느 구역에도 넣기 애매할 때는 reject 하는 option 을 택할 수도 있다.


우리는 지금까지 분류 모델을 두 개의 단계로 나눠 풀었다. 1. inference stage 를 통해 posterior $p(C_k \vert x)$ 를 추론하고 2. decision stage 를 통해 최종 target 을 예측하는 방법을 살펴 봤다. 그런데 사실 결정문제를 푸는데는 세가지 다른 방법이 있다.
1. generative model
1. discriminative model
1. 확률 계산 안하고 그냥 input 넣으면 바로 class 나오는 model ~~(뭐라 불러야 해 이걸)~~

각각에 대한 설명

1. generative model  
각각의 class $C_k$ 에 대한 조건부 확률 밀도 $p(x \vert C_k)$ 를 inference stage 로 풀어낸다. 그리고 각 class 에 대한 $p(C_k)$ 도 따로 구한다. 이를 이용하여 bayesian 정리를 통해 posterior $p(C_k \vert x)$ 를 계산한다.  
$p(C_k \vert x) = \frac{p(x \vert C_k)p(C_k)}{p(x)}$. 분모는 $p(x)=\sum p(x \vert C_k)p(C_k)$.  
이렇게 posterior를 알아낸후에는 decision theory 를 이용하여 class 를 예측한다.  
이는 generative model 이라 불리는데 위에서 구한 $p(x)$ 를 통해 input space 의 합성 data를 생성할 수 있기 때문이다.
2. discriminative model  
위에서 열심히 했던 것이다. posterior $p(C_k \vert x)$ 를 inference stage 로 추론하고 decision theory 이용해서 결정. 이땐 $p(C_k)$ 등의 분포는 관심대상이 아니다.
3. 설명 생략 ex) $x \in [0, 1]$ 인 binary classificatoin 문제를 생각하자. x가 0.5 이상이면 1, 0.5 이하면 0 를 출력하는 모델이 예시가 되겠다.

당연히 discriminative model 가 generative model 보다 class 예측하는 데에선 효과적이다.

3의 방법을 쓰면 posterior 를 계산하지 않기 때문에 risk 관리가 힘들어 금융에서 사용하기 안 좋다.

input 이 여러개면 어떡할까? 간단히 독립이라 가정하는 방법이 있다.(naive bayes model)  
$p(x1, x2 \vert C_k) = p(x1 \vert C_k) p(x2 \vert C_k)$  
사후확률은 $p(C_k \vert x1, x2) \propto \frac{p(C_k \vert x1)p(C_k \vert x2)}{p(C_k)}$ 가 된다.

지금까진 classification 에서 decision theory 를 함.  
이제 regression 에서 decision theory 해보자.

regression 에서는 각 x에 대해 t의 추정값 y(x) 를 구해야 한다.  

regression function: $f(x)= \mathbb{E}\left[t \vert x \right]$  

각각의 x마다 loss $L(t, y(t))$ 를 생각할 수 있다. 그러면 평균 Loss 는 다음과 같을 것이다.

$$\mathbb{E}\left[ L\right] = \int \int L(t, y(t))p(x, t)dxdt$$

일반적으로 regression 에서는 $L(t, y(t))=\left(y(x)-t\right)^2$ 를 사용한다. 이를 적용하면..

$$\mathbb{E}\left[ L\right] = \int \int (t-y(t))^2 p(x, t)dxdt$$

우리의 목표는 E를 최소화하는 y를 구하는 것이므로 $y(t)$로 양변 미분하면

$$\frac{\partial \mathbb{E}[L]}{\partial y(x)} = 2 \int [y(t)-t]p(x, t)dt = 0$$

이를 정리하면

$$y(x) = \frac{\int t p(x, t)dt}{p(x)} = \int t p(t \vert x)dt = \mathbb{E}_t [t \vert x]$$

$$\therefore f(x)= \mathbb{E}_t \left[t \vert x \right]$$

regression 을 푸는 방법도 여러가지 있음  
1. $p(x, t)$ 풀고 이를 통해 $p(t \vert x)$ 구하고 $\mathbb{E}\left[t \vert x \right]$ 계산
2. $p(t \vert x)$ 구하고 $\mathbb{E}_t \left[t \vert x \right]$ 계산
3. $y(x)$ 바로 계산

위에서 우리는 regression 의 loss 로 l2 loss 를 사용했는데 이는 여러 상황에서 적절하지 않다. 예를 들어 multi-modal distribution 인 상황이 있다.

l2 loss 를 generalize 한 loss 로 Minkowski loss 가 있다.

$$\mathbb{E}\left[L_q\right] = \int \int \left\vert y(x) - t \right\vert^q p(x, t) dx dt$$

$\mathbb{E}\left[L_q\right]$ 를 최소화 하는 y(x) 는 q = 2 이면 (x에 대한) 조건부 평균, q = 1이면 조건부 중앙값, q = 0이면 조건부 최빈값일 때 최소가 된다.

# 1.6 Information Theory

정보량 $h(x)$ 은 $p(x)$ 에 단조 감소이고, 서로 연관되지 않은 두 사건 x와 y에 대해 $h(x, y) = h(x) + h(y)$ 이다.  
$h(x) = -\log_2p(x)$ 가 이를 만족한다.  
**엔트로피** := **정보량의 평균치** = $H\left[x\right] = - \sum_x p(x) \log_2 p(x)$ 이다.  
예시로 class 가 n개고 균등 분포인 걸 생각하면 $H\left[x\right] = -n * \frac{1}{n} \log_2 \frac{1}{n} = \log_2 n$

shannon 에 의하면 엔트로피는 확률변수의 상태를 전송하기 위해 필요한 비트 숫자의 하한선이다.(noiseless coding theorem)

앞으론 $\log_2$ 대신 $\log$ 사용

conditional entropy:

$$\begin{matrix}
H[x, y] &=& -\int \log p(x, y) p(x, y) dxdy\\
& =& -\int \log \left[p(y \vert x) p(x)\right] p(x, y) dx dy\\
& =& -\int \log p(y \vert x) p(x, y)dydx - \int \log p(x) p(x) dx\\
& =& H[y \vert x] + H[x]
\end{matrix} $$

**KL divergence (= relative entropy)**  

$$\begin{matrix}
KL(p \parallel q) &=& -\int p(x) \log q(x) dx - \left(-\int p(x) \log p(x) dx\right)\\
&=&-\int p(x) \log \frac{q(x)}{p(x)} dx
\end{matrix}$$