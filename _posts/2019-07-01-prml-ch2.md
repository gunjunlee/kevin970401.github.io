---
layout: post
title: "PRML Ch.2 - probability distribution"
categories: PRML
author: lee gunjun
---

prml 을 공부한 사람을 위한 글입니다.  
공부하실 분들에겐 더 좋은 블로그가 있으니 추천드립니다. [norman3 님 블로그](http://norman3.github.io/prml/)

density estimation: observed set $x_1, \cdots, x_n$ 이 주어졌을 때 $p(x)$ 을 모델링 하는 것.

conjugate prior probability, exponential family 

$D = {x_1, \cdots, x_N}$ 이라 하자.

Bernoulli: $Bern(x \vert \mu) = \mu^x(1-\mu)^{1-x} \text{, x}\in\{0,1\}$  
$\log p(D \vert \mu) = \sum_{n=1}^N \left\{x_n\log\mu + (1-x_n) \log(1-\mu) \right\}$  
$\mu^{MLE} = \frac{1}{N} \sum_{n=1}^N x_n$

Binomial: $Bin(m \vert N, \mu) = \binom{N}{m}\mu^m(1-\mu)^{N-m}$

Bernoulli 와 Binomial 의 문제점: N=3 밖에 안되고 모두 x=0 이었다고 하자. 이 때 MLE 을 쓰면 미래에도 계속 0 이 나올것이라 생각함. 즉 쉽게 overfitting 된다. bayesian 처럼 해보기 위해 $\mu$의 사전분포를 도입해보자.

Beta: $Beta(\mu \vert a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}$

$\mu$ 의 prior 를 beta 라 하고 bernoulli 의 likelihood 와 곱하면  
$p(\mu \vert m, N, a, b) \propto p(D \vert \mu) p(\mu) \propto \mu^{m+a-1} (1-\mu)^{N-m+b-1}$  
$\therefore p(\mu \vert m, l, a, b) \sim Beta(\mu \vert m+a, l+b),\ l=N-m$  
prior 를 beta 로 잡아서 posterior 를 계산했는데 이 또한 beta 가 됐다. 이러한 성질을 conjugacy 라 한다.

위와 같은 상황처럼 사전 분포를 잘 모델링 하면 매 관찰마다 얻어낸 사후 분포를 그 다음 관찰에 대해 사전분포로 사용할 수 있다. 이렇게 매번 업데이트 하는 것이 bayesian 적 관점에서 자연스럽다. 이는 사전 분포나 가능도 함수의 선택과는 관련없이 데이터가 iid 이면 된다.  

더 많은 데이터를 관측할수록 posterior 의 불확실성 정도가 줄어들까? 즉 다시말해 posterior 의 분산이 줄어들까?

$$\begin{matrix}
var_D\left[\mathbb{E}_\theta \left[ \theta \vert D \right] \right] &=& \mathbb{E}_D \left[\mathbb{E}_\theta\left[\theta \vert D\right]^2\right] - \mathbb{E}_D \left[\mathbb{E}_\theta\left[\theta \vert D\right]\right]^2\\
&=&\mathbb{E}_D \left[\mathbb{E}_\theta\left[\theta \vert D\right]^2\right] - \mathbb{E}_\theta\left[\theta\right]^2\\
&=&\mathbb{E}_D \left[\mathbb{E}_\theta\left[\theta \vert D\right]^2\right] + var_\theta\left[\theta\right] -\mathbb{E}_\theta\left[\theta^2\right]\\
&=&\mathbb{E}_D \left[\mathbb{E}_\theta\left[\theta \vert D\right]^2\right] + var_\theta\left[\theta\right] -\mathbb{E}_D\left[\mathbb{E}_\theta\left[\theta^2 \vert D\right]\right]\\
&=& -\mathbb{E}_D\left[var_\theta\left[\theta \vert D\right]\right] + var_\theta\left[\theta\right]
\end{matrix}$$

사후 평균의 분산은 사전 분산 - 사후 분산의 평균으로 나타내어진다. 즉 분산이 줄어든다! 물론 여기서 본건 사후 *평균*의 분산이므로 어쩔땐 커지기도 한다.

지금까지 binomial 의 사전분포로 beta 를 쓰는 걸 했다.  
multinomial 의 경우는 사전분포로 어떤 걸 써야할까?  
아마 자연스럽게 dirichlet 을 써야한다고 생각할 것이다. 그리고 그게 정답이다.

multinomial: $Mult(m_1, \cdots, m_K \vert \mu, N) = \binom{N}{m_1 \cdots m_K}\prod_{k=1}^K\mu_k^{m_n}$, $\sum_{k=1}^K m_k=N$

likelihood: $p(D \vert \mu) = \prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{\sum_n x_{nk}} = \prod_{k=1}^K \mu_k^{m_k}$

사전분포로 다음과 같이 잡고 싶을 것이다.  
$p(\mu \vert \alpha) \propto \prod_{k=1}^K \mu_k^{\alpha_k - 1}$  
normalize 하면  
dirichlet: $Dir(\mu \vert \alpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) \cdots \Gamma(\alpha_K)} \prod_{k=1}^K \mu_k^{\alpha_k - 1}$, $\alpha_0 = \sum_{k=1}^K \alpha_k$

dirichlet 을 prior 로 쓰고, likelihood 를 곱하면 사후분포가 다음과 같이 구해진다.  
$p(\mu \vert D, \alpha) \propto p(D \vert \mu) p(\mu \vert \alpha) \propto \prod_{k=1}^K \mu_k^{\alpha_k + m_k - 1}$  
우리가 원하는 대로 사후 분포 또한 dirichlet 을 따른다. normalize 하면 다음과 같은 식이 얻어진다.  
$p(\mu \vert D, \alpha) = Dir(\mu \vert \alpha+m)$

지금까진 discrete 변수를 모델링하는 분포였다.  
연속 변수를 모델링 해보자

Gaussian: $\mathcal{N}(x \vert \mu, \sigma^2) = \frac{1}{(2 \pi \sigma^2)^{1/2}}\exp\left\{- \frac{1}{2\sigma^2}(x-\mu)^2\right\}$  
D-dim Gaussian: $\mathcal{N}(x \vert \mu, \Sigma) = \frac{1}{(2\pi)^{D/2} \left\vert \Sigma \right\vert^{1/2}} \exp\left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\}$

참고로 gaussian 은 중요한 해석적 성질을 많이 가졌다.  
첫째로 Mahalanobis distance $\Delta^2 = (x-\mu)^T \Sigma^{-1} (x-\mu)$  
$\Sigma=\Sigma^{sym} + \Sigma^{antisym}$로 나누면 Mahaloanobis distance 에서 $\Sigma^{antisym}$ 은 사라지므로 generality 을 잃지 않고 우리는 $\Sigma$ 가 대칭행렬이라 할 수 있다.

실수 대칭 행렬이므로 고유값 또한 실수다. 정규 직교 집합을 이루는 고유 벡터($u_i^Tu_j=I_{ij}$)를 선택하면 $\Sigma = \sum_{i=1}^D \lambda_i u_i u_u^T$ 가 될것이고 $\Sigma^{-1} = \sum_{i=1}^D \frac{1}{\lambda_i}u_iu_i^T$ 이다. 이를 mahalanobis distance 에 적용하면 $\Delta^2 = \sum_{i=1}^D \frac{y_i^2}{\lambda_i},\ y_i = u_i^T (x-\mu)$ 식으로 해서 그렇지 그림(Figure 2.7) 보면 쉽다.
